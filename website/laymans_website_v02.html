<!DOCTYPE html>

<html>

<head>

<title>Following Bertsimas</title>

</head>

<body>

<h2>Following Bertsimas:  Machine Learning through
a modern optimization lens</h2>



Student(s): Sebastiaan Higler, Steffen Schneider,
Krist Shingjergji, Morris Stallmann, Jonas Schmidt;

Supervisor(s): Steven Kelk, Matus Mihalak, Georgios Stamoulis;

Semester: Spring 2018;



<figure><img style="float: left; padding-right: 20px;"

src="./fig1_decision_tree.png" alt="" width="458" height="300" />

<figcaption>Fig 1. - An optimal decision tree

</figcaption>

</figure>



<h4>Problem statement and motivation:</h4>

<p align=”justify”>
Decision trees are graphs that illustrate consequences of decisions. Starting from the top, at every node a decision has to be made until the bottom of the tree is reached and a final outcome can be inferred. <br> 

In the Machine Learning (ML) community, such trees are often used to classify data. [ADD SEMI-FUNNY EXAMPLE HERE (also explain split and branch)]... <br>

In particular, a rich body of research in ML is concerned with constructing those trees automatically from data and one of the most famous algorithms is CART [reference here]. While being extremly popular and succesful, CART shares a disadvantage with many other tree-construction-methods: They are heuristics. That means, these techniques can neither guarantee to construct trees, that minimize the total number of misclassified datapoints nor to construct trees that generalize well to unseen data. <br>

Dimitras Bertsimas is a professor and mathematician at the Massachusetts Institute of Technology (MIT) and researches how mathematical optimization can improve ML-algorithms. Among other applications, he is concerned with creating optimal classification trees. He proposes to reformulate the task of creating an optimal decision tree as a mathematical one, namely as a Mixed Integer Optimization (MIO) problem. The reformulation is not a heuristic anymore and can guarantee constructing an optimal tree. Moreover, he introduces a generalization and novel approach, that allows constructing trees splitting on multiple variables at the same time. However, the optimality comes at a price: Significantly increased computational intensity.<br>

Bertsimas claims that his methods outperform the famous CART-heuristic and are computational feasible. This project is concerned with validating those claims by verifying the soundness of the approach, implementing the model and running experiments with a variety of datasets. <br>


<figure><img style="text-align: center;" src="http://<url to image>" alt="" />

<figcaption>Fig 2. - ???.

</figcaption>

</figure>



<figure><img style="text-align: center;" src="http://<url to image>" alt="" />

<figcaption>Fig 3. - ???.

</figcaption>

</figure>



<h4>Research questions/hypotheses:</h4>

<ul>

<li>Can the algorithm be implemented as described by Bertsimas?</li>

<li>Can we reproduce Bertsimas experimential results concerning OCT and OCT-H?</li>

<li>How does our implementations perform compared to Bertsima's and compared to the commonly used CART?</li>

</ul>



<h4>Main outcomes:</h4>

<ul>

<li>???.</li>

<li>???.</li>

</ul>



<h4>References:</h4>

<p>D. Bertsimas and J. Dunn, "Optimal classification trees", Machine Learning, vol. 106, no. 7, pp. 1039-1082, Jul. 2017</p>
<p>L. Breiman, J. Friedman, C. Stone, and R. Olshen, "Classification and Regression Trees", ser. The Wadsworth and Brooks-Cole statistics-probability series. Taylor & Francis, 1984</p>
TODO Reference for:
<p>Integer Linear Programming</p>


<h4>Downloads:</h4>

<a href=”???” target=”_blank”>Final report</a>

<a href=”???” target=”_blank”>Final presentation</a>



</body>

</html>
